{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11465344,"sourceType":"datasetVersion","datasetId":7184780},{"sourceId":11467726,"sourceType":"datasetVersion","datasetId":7186402}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a02285e6","cell_type":"markdown","source":"# Starter Notebook","metadata":{"id":"a02285e6"}},{"id":"bdcc5329","cell_type":"markdown","source":"Install and import required libraries","metadata":{"id":"bdcc5329"}},{"id":"348ceed6-b684-46c3-8a32-9bb640c9a9d7","cell_type":"code","source":"!pip install datasets evaluate accelerate peft trl bitsandbytes\n!pip install nvidia-ml-py3\n!pip install transformers --upgrade","metadata":{"id":"348ceed6-b684-46c3-8a32-9bb640c9a9d7","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:50:51.756565Z","iopub.execute_input":"2025-04-18T21:50:51.756775Z","iopub.status.idle":"2025-04-18T21:52:28.896562Z","shell.execute_reply.started":"2025-04-18T21:50:51.756758Z","shell.execute_reply":"2025-04-18T21:52:28.895717Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting trl\n  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.16.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl, evaluate, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 evaluate-0.4.3 fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.16.1\nCollecting nvidia-ml-py3\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: nvidia-ml-py3\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19173 sha256=c3669aa2332b8c861cef166ed55f4d22b56ed92257ab5e371e8aed24e728a38c\n  Stored in directory: /root/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\nSuccessfully built nvidia-ml-py3\nInstalling collected packages: nvidia-ml-py3\nSuccessfully installed nvidia-ml-py3-7.352.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\nSuccessfully installed transformers-4.51.3\n","output_type":"stream"}],"execution_count":1},{"id":"cca64f38-d8d2-4313-8295-fbbd43c2a263","cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nfrom peft import LoraConfig, get_peft_model\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport evaluate","metadata":{"id":"cca64f38-d8d2-4313-8295-fbbd43c2a263","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:52:28.897549Z","iopub.execute_input":"2025-04-18T21:52:28.897790Z","iopub.status.idle":"2025-04-18T21:52:53.841253Z","shell.execute_reply.started":"2025-04-18T21:52:28.897756Z","shell.execute_reply":"2025-04-18T21:52:53.840313Z"}},"outputs":[{"name":"stderr","text":"2025-04-18 21:52:41.104245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745013161.320016      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745013161.381253      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"id":"59d6e377","cell_type":"markdown","source":"## Load Tokenizer and Preprocess Data","metadata":{"id":"59d6e377"}},{"id":"21f42747-f551-40a5-a95f-7affb1eba4a3","cell_type":"code","source":"base_model = \"roberta-base\"\ndataset = load_dataset('ag_news', split='train')\ntokenizer = RobertaTokenizer.from_pretrained(base_model)\n\ndef preprocess(examples):\n    return tokenizer(examples['text'], truncation=True, padding=True)\ndataset = dataset.filter(lambda x: 10 < len(x[\"text\"].split()) <= 512)\n\ntokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\ntokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n\n# Class info\nnum_labels = dataset.features['label'].num_classes\nid2label = {i: label for i, label in enumerate(dataset.features['label'].names)}\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n","metadata":{"id":"21f42747-f551-40a5-a95f-7affb1eba4a3","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:52:53.843492Z","iopub.execute_input":"2025-04-18T21:52:53.844292Z","iopub.status.idle":"2025-04-18T21:53:56.873854Z","shell.execute_reply.started":"2025-04-18T21:52:53.844257Z","shell.execute_reply":"2025-04-18T21:53:56.873245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b71a73e2f04119b3caef9889078bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77edb40d7bd47cba3bf77a6d191c5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a8b002b1fb401e920aa36af59951e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4174857977ad4e6cba37428eec353934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395c075b384649e887f69ef84a8d1d29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6947534ce8854122bf153f1160c57595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e692ad49164d35b04b918e20d84004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd378d847cb44f9aa24a4eac52617b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884c2d559c1847a083b7b2bb1ed86db1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e11774fe9d04a7babbdc0976b30f584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0639f01447e4557b8f80e931bf170a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119985 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de35456ae15341679070b40c296100e1"}},"metadata":{}}],"execution_count":3},{"id":"9e07f641-bec0-43a6-8c26-510d7642916a","cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\n    base_model, num_labels=num_labels, id2label=id2label\n)\n\npeft_config = LoraConfig(\n    r=6,  # adjust this and alpha if needed\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=[\"query\", \"value\", \"key\"],  # key modules for attention\n    task_type=\"SEQ_CLS\"\n)\n\npeft_model = get_peft_model(model, peft_config)\npeft_model.print_trainable_parameters()\n","metadata":{"id":"9e07f641-bec0-43a6-8c26-510d7642916a","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:55:42.461877Z","iopub.execute_input":"2025-04-18T21:55:42.462641Z","iopub.status.idle":"2025-04-18T21:55:42.707615Z","shell.execute_reply.started":"2025-04-18T21:55:42.462609Z","shell.execute_reply":"2025-04-18T21:55:42.706872Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n","output_type":"stream"}],"execution_count":7},{"id":"f265839d-a088-4693-8474-862641de11ed","cell_type":"markdown","source":"## Anything from here on can be modified","metadata":{"id":"f265839d-a088-4693-8474-862641de11ed"}},{"id":"e7413430-be57-482b-856e-36bd4ba799df","cell_type":"code","source":"split = tokenized_dataset.train_test_split(test_size=640, seed=42)\ntrain_dataset = split['train']\neval_dataset = split['test']\n","metadata":{"id":"e7413430-be57-482b-856e-36bd4ba799df","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:55:44.395772Z","iopub.execute_input":"2025-04-18T21:55:44.396090Z","iopub.status.idle":"2025-04-18T21:55:44.449254Z","shell.execute_reply.started":"2025-04-18T21:55:44.396064Z","shell.execute_reply":"2025-04-18T21:55:44.448712Z"}},"outputs":[],"execution_count":8},{"id":"12284b58","cell_type":"markdown","source":"## Training Setup","metadata":{"id":"12284b58"}},{"id":"0ee64c43-fe38-479a-b3c5-7d939a3db4c1","cell_type":"code","source":"def compute_metrics(pred):\n    preds = pred.predictions.argmax(-1)\n    return {\"accuracy\": accuracy_score(pred.label_ids, preds)}\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    learning_rate=2e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator\n)\n","metadata":{"id":"0ee64c43-fe38-479a-b3c5-7d939a3db4c1","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:55:45.994550Z","iopub.execute_input":"2025-04-18T21:55:45.995236Z","iopub.status.idle":"2025-04-18T21:55:46.411739Z","shell.execute_reply.started":"2025-04-18T21:55:45.995211Z","shell.execute_reply":"2025-04-18T21:55:46.411079Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1854385982.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":9},{"id":"9b848278","cell_type":"markdown","source":"### Start Training","metadata":{"id":"9b848278"}},{"id":"98d9d57d-b57f-4acc-80fb-fc5443e75515","cell_type":"code","source":"trainer.train()\n","metadata":{"id":"98d9d57d-b57f-4acc-80fb-fc5443e75515","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T21:55:48.792645Z","iopub.execute_input":"2025-04-18T21:55:48.793188Z","iopub.status.idle":"2025-04-19T01:29:59.707613Z","shell.execute_reply.started":"2025-04-18T21:55:48.793167Z","shell.execute_reply":"2025-04-19T01:29:59.706869Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14920' max='14920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14920/14920 3:34:08, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.269800</td>\n      <td>0.252297</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.250500</td>\n      <td>0.221683</td>\n      <td>0.923438</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.226800</td>\n      <td>0.209723</td>\n      <td>0.928125</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.223400</td>\n      <td>0.205723</td>\n      <td>0.926562</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=14920, training_loss=0.2992374527550255, metrics={'train_runtime': 12850.4997, 'train_samples_per_second': 37.149, 'train_steps_per_second': 1.161, 'total_flos': 8.418353115112325e+16, 'train_loss': 0.2992374527550255, 'epoch': 4.0})"},"metadata":{}}],"execution_count":10},{"id":"5183be7e-514f-4e64-a6f4-314a827e6be5","cell_type":"markdown","source":"## Evaluate Finetuned Model\n","metadata":{"id":"5183be7e-514f-4e64-a6f4-314a827e6be5"}},{"id":"665e032c-4ef7-48dd-8d35-0e3e30a4a9a5","cell_type":"code","source":"print(\"📊 Final Evaluation:\")\ntrainer.evaluate()\n\ntrainable = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\nprint(f\"✅ Total Trainable Parameters: {trainable}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:29:59.708969Z","iopub.execute_input":"2025-04-19T01:29:59.709417Z","iopub.status.idle":"2025-04-19T01:30:06.336236Z","shell.execute_reply.started":"2025-04-19T01:29:59.709398Z","shell.execute_reply":"2025-04-19T01:30:06.335445Z"}},"outputs":[{"name":"stdout","text":"📊 Final Evaluation:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"✅ Total Trainable Parameters: 925444\n","output_type":"stream"}],"execution_count":11},{"id":"038198cf-0953-47e7-bd47-b073d05f8378","cell_type":"markdown","source":"### Performing Inference on Custom Input\nUncomment following functions for running inference on custom inputs","metadata":{"id":"038198cf-0953-47e7-bd47-b073d05f8378"}},{"id":"f88ad420-3f46-4eff-9d71-0ce388163062","cell_type":"code","source":"def classify(model, tokenizer, text):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    output = model(**inputs)\n\n    prediction = output.logits.argmax(dim=-1).item()\n\n    print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n    return id2label[prediction]","metadata":{"id":"f88ad420-3f46-4eff-9d71-0ce388163062","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:30:06.336972Z","iopub.execute_input":"2025-04-19T01:30:06.337192Z","iopub.status.idle":"2025-04-19T01:30:06.341993Z","shell.execute_reply.started":"2025-04-19T01:30:06.337174Z","shell.execute_reply":"2025-04-19T01:30:06.341026Z"}},"outputs":[],"execution_count":12},{"id":"fc52bb94-5e13-4943-9225-a6d7fd053579","cell_type":"code","source":"classify( peft_model, tokenizer, \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\nclassify( peft_model, tokenizer, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")","metadata":{"id":"fc52bb94-5e13-4943-9225-a6d7fd053579","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:30:06.343613Z","iopub.execute_input":"2025-04-19T01:30:06.344057Z","iopub.status.idle":"2025-04-19T01:30:06.425167Z","shell.execute_reply.started":"2025-04-19T01:30:06.344039Z","shell.execute_reply":"2025-04-19T01:30:06.424641Z"}},"outputs":[{"name":"stdout","text":"\n Class: 1, Label: Sports, Text: Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\n\n Class: 2, Label: Business, Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindlinand of ultra-cynics, are seeing green again.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'Business'"},"metadata":{}}],"execution_count":13},{"id":"68a3e276-bf8c-4403-8a48-5ef19f2beccf","cell_type":"markdown","source":"### Run Inference on eval_dataset","metadata":{"id":"68a3e276-bf8c-4403-8a48-5ef19f2beccf"}},{"id":"ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5","cell_type":"code","source":"from torch.utils.data import DataLoader\nimport evaluate\nfrom tqdm import tqdm\n\ndef evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n    \"\"\"\n    Evaluate a PEFT model on a dataset.\n\n    Args:\n        inference_model: The model to evaluate.\n        dataset: The dataset (Hugging Face Dataset) to run inference on.\n        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n                         If False, only predictions will be returned.\n        batch_size (int): Batch size for inference.\n        data_collator: Function to collate batches. If None, the default collate_fn is used.\n\n    Returns:\n        If labelled is True, returns a tuple (metrics, predictions)\n        If labelled is False, returns the predictions.\n    \"\"\"\n    # Create the DataLoader\n    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    inference_model.to(device)\n    inference_model.eval()\n\n    all_predictions = []\n    if labelled:\n        metric = evaluate.load('accuracy')\n\n    # Loop over the DataLoader\n    for batch in tqdm(eval_dataloader):\n        # Move each tensor in the batch to the device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = inference_model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)\n        all_predictions.append(predictions.cpu())\n\n        if labelled:\n            # Expecting that labels are provided under the \"labels\" key.\n            references = batch[\"labels\"]\n            metric.add_batch(\n                predictions=predictions.cpu().numpy(),\n                references=references.cpu().numpy()\n            )\n\n    # Concatenate predictions from all batches\n    all_predictions = torch.cat(all_predictions, dim=0)\n\n    if labelled:\n        eval_metric = metric.compute()\n        print(\"Evaluation Metric:\", eval_metric)\n        return eval_metric, all_predictions\n    else:\n        return all_predictions","metadata":{"id":"ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:30:06.425774Z","iopub.execute_input":"2025-04-19T01:30:06.425947Z","iopub.status.idle":"2025-04-19T01:30:06.432496Z","shell.execute_reply.started":"2025-04-19T01:30:06.425933Z","shell.execute_reply":"2025-04-19T01:30:06.431946Z"}},"outputs":[],"execution_count":14},{"id":"809635a6-a2c7-4d09-8d60-ababd1815003","cell_type":"code","source":"# Check evaluation accuracy\n_, _ = evaluate_model(peft_model, eval_dataset, True, 8, data_collator)","metadata":{"id":"809635a6-a2c7-4d09-8d60-ababd1815003","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:30:06.433256Z","iopub.execute_input":"2025-04-19T01:30:06.433481Z","iopub.status.idle":"2025-04-19T01:30:18.371947Z","shell.execute_reply.started":"2025-04-19T01:30:06.433461Z","shell.execute_reply":"2025-04-19T01:30:18.371230Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e9b2902c5c49dc82fed7dbac5a481e"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 80/80 [00:11<00:00,  6.98it/s]","output_type":"stream"},{"name":"stdout","text":"Evaluation Metric: {'accuracy': 0.928125}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"id":"kMJgvV1ZnVhd","cell_type":"code","source":"","metadata":{"id":"kMJgvV1ZnVhd","trusted":true},"outputs":[],"execution_count":null},{"id":"75f39087-f2bb-49d3-9fe1-0d812fb30203","cell_type":"markdown","source":"### Run Inference on unlabelled dataset","metadata":{"id":"75f39087-f2bb-49d3-9fe1-0d812fb30203"}},{"id":"3461de4a-c4bd-44a4-8528-7e131e64f25b","cell_type":"code","source":"#Load your unlabelled data\nunlabelled_dataset = pd.read_pickle(\"/kaggle/input/test-unlabelled-pkl/test_unlabelled.pkl\")\ntest_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\nunlabelled_dataset\n# Run inference and save predictions\npreds = evaluate_model(peft_model, test_dataset, False, 8, data_collator)\ndf_output = pd.DataFrame({\n    'ID': range(len(preds)),\n    'Label': preds.numpy()  # or preds.tolist()\n})\ndf_output.to_csv(\"/kaggle/working/inference_output2.csv\", index=False)\nprint(\"Inference complete. Predictions saved to inference_output.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T01:55:43.069245Z","iopub.execute_input":"2025-04-19T01:55:43.069517Z","iopub.status.idle":"2025-04-19T01:57:41.001874Z","shell.execute_reply.started":"2025-04-19T01:55:43.069495Z","shell.execute_reply":"2025-04-19T01:57:41.000989Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e57a6d00bf264514b313fefa98800393"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1000/1000 [01:51<00:00,  8.93it/s]","output_type":"stream"},{"name":"stdout","text":"Inference complete. Predictions saved to inference_output.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18},{"id":"5855ae4b-6e9a-447b-b51e-c8f65c185ecc","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e10a2ea7-fa96-4426-ad25-61c1eba7e6a9","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4f2ef036-6acc-402a-a859-c367d87ef3a4","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a87fb31f-4c8c-47dc-80aa-b478b3e53c14","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9e38497a-6c89-4c4a-8be5-8cc08e53860f","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c3e9c3e7-2adf-445f-8877-463311d04cf4","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"144fe756-0139-48b0-867b-a1e93ea9e036","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7a8114a6-0600-4ab1-96c1-e14878d29a3c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d84aac05-2e1d-40ff-a55c-a32614da13b8","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e4550c64-856a-4ee6-95a7-0d9f1627a090","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e0fa8fc1-e99b-4501-8f74-451272e0b2fd","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"68f87eb0-cc6d-419c-b78a-99c603222e76","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3f784897-241b-4522-b5f4-4583c1c17198","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3ef406ae-f88e-4b4c-ab4a-7982af3f4b67","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"52402029-3db1-49bb-8655-f276af25bd0b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f244b3f1-06a4-4d2d-bd7a-44585325d7fc","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}